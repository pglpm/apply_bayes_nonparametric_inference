\pdfoutput=1
%% Author: PGL  Porta Mana
%% Created: 2015-05-01T20:53:34+0200
%% Last-Updated: 2021-11-18T15:24:42+0100
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newif\ifarxiv
\arxivfalse
\ifarxiv\pdfmapfile{+classico.map}\fi
\newif\ifafour
\afourfalse% true = A4, false = A5
\newif\iftypodisclaim % typographical disclaim on the side
\typodisclaimtrue
\newcommand*{\memfontfamily}{zplx}
\newcommand*{\memfontpack}{newpxtext}
\documentclass[\ifafour a4paper,12pt,\else a5paper,10pt,\fi%extrafontsizes,%
onecolumn,oneside,article,%french,italian,german,swedish,latin,
british%
]{memoir}
\newcommand*{\firstdraft}{16 June 2021}
\newcommand*{\firstpublished}{\firstdraft}
\newcommand*{\updated}{\ifarxiv***\else\today\fi}
\newcommand*{\propertitle}{Notes on RMSD inference\\{\large for Alexander,
    Kjetil, Ruth}%
}% title uses LARGE; set Large for smaller
\newcommand*{\pdftitle}{Notes on RMSD inference}
\newcommand*{\headtitle}{RMSD inference}
\newcommand*{\pdfauthor}{P.G.L.  Porta Mana}
\newcommand*{\headauthor}{Porta Mana}
\newcommand*{\reporthead}{\iftrue\else Open Science Framework \href{https://doi.org/10.31219/osf.io/***}{\textsc{doi}:10.31219/osf.io/***}\fi}% Report number

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Calls to packages (uncomment as needed)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\usepackage{pifont}

%\usepackage{fontawesome}

\usepackage[T1]{fontenc}
\input{glyphtounicode} \pdfgentounicode=1

\usepackage[utf8]{inputenx}

%\usepackage{newunicodechar}
% \newunicodechar{Ĕ}{\u{E}}
% \newunicodechar{ĕ}{\u{e}}
% \newunicodechar{Ĭ}{\u{I}}
% \newunicodechar{ĭ}{\u{\i}}
% \newunicodechar{Ŏ}{\u{O}}
% \newunicodechar{ŏ}{\u{o}}
% \newunicodechar{Ŭ}{\u{U}}
% \newunicodechar{ŭ}{\u{u}}
% \newunicodechar{Ā}{\=A}
% \newunicodechar{ā}{\=a}
% \newunicodechar{Ē}{\=E}
% \newunicodechar{ē}{\=e}
% \newunicodechar{Ī}{\=I}
% \newunicodechar{ī}{\={\i}}
% \newunicodechar{Ō}{\=O}
% \newunicodechar{ō}{\=o}
% \newunicodechar{Ū}{\=U}
% \newunicodechar{ū}{\=u}
% \newunicodechar{Ȳ}{\=Y}
% \newunicodechar{ȳ}{\=y}

\newcommand*{\bmmax}{0} % reduce number of bold fonts, before font packages
\newcommand*{\hmmax}{0} % reduce number of heavy fonts, before font packages

\usepackage{textcomp}

%\usepackage[normalem]{ulem}% package for underlining
% \makeatletter
% \def\ssout{\bgroup \ULdepth=-.35ex%\UL@setULdepth
%  \markoverwith{\lower\ULdepth\hbox
%    {\kern-.03em\vbox{\hrule width.2em\kern1.2\p@\hrule}\kern-.03em}}%
%  \ULon}
% \makeatother

\usepackage{amsmath}

\usepackage{mathtools}
%\addtolength{\jot}{\jot} % increase spacing in multiline formulae
\setlength{\multlinegap}{0pt}

%\usepackage{empheq}% automatically calls amsmath and mathtools
%\newcommand*{\widefbox}[1]{\fbox{\hspace{1em}#1\hspace{1em}}}

%%%% empheq above seems more versatile than these:
%\usepackage{fancybox}
%\usepackage{framed}

% \usepackage[misc]{ifsym} % for dice
% \newcommand*{\diceone}{{\scriptsize\Cube{1}}}

\usepackage{amssymb}

\usepackage{amsxtra}

\usepackage[main=british]{babel}\selectlanguage{british}
%\newcommand*{\langnohyph}{\foreignlanguage{nohyphenation}}
\newcommand{\langnohyph}[1]{\begin{hyphenrules}{nohyphenation}#1\end{hyphenrules}}

\usepackage[autostyle=false,autopunct=false,english=british]{csquotes}
\setquotestyle{american}
\newcommand*{\defquote}[1]{`\,#1\,'}

% \makeatletter
% \renewenvironment{quotation}%
%                {\list{}{\listparindent 1.5em%
%                         \itemindent    \listparindent
%                         \rightmargin=1em   \leftmargin=1em
%                         \parsep        \z@ \@plus\p@}%
%                 \item[]\footnotesize}%
%                 {\endlist}
% \makeatother


\usepackage{amsthm}
%% from https://tex.stackexchange.com/a/404680/97039
\makeatletter
\def\@endtheorem{\endtrivlist}
\makeatother

\newcommand*{\QED}{\textsc{q.e.d.}}
\renewcommand*{\qedsymbol}{\QED}
\theoremstyle{remark}
\newtheorem{note}{Note}
\newtheorem*{remark}{Note}
\newtheoremstyle{innote}{\parsep}{\parsep}{\footnotesize}{}{}{}{0pt}{}
\theoremstyle{innote}
\newtheorem*{innote}{}

\usepackage[shortlabels,inline]{enumitem}
\SetEnumitemKey{para}{itemindent=\parindent,leftmargin=0pt,listparindent=\parindent,parsep=0pt,itemsep=\topsep}
% \begin{asparaenum} = \begin{enumerate}[para]
% \begin{inparaenum} = \begin{enumerate*}
\setlist{itemsep=0pt,topsep=\parsep}
\setlist[enumerate,2]{label=\alph*.}
\setlist[enumerate]{label=\arabic*.,leftmargin=1.5\parindent}
\setlist[itemize]{leftmargin=1.5\parindent}
\setlist[description]{leftmargin=1.5\parindent}
% old alternative:
% \setlist[enumerate,2]{label=\alph*.}
% \setlist[enumerate]{leftmargin=\parindent}
% \setlist[itemize]{leftmargin=\parindent}
% \setlist[description]{leftmargin=\parindent}

\usepackage[babel,theoremfont,largesc]{newpxtext}

\usepackage[bigdelims,nosymbolsc%,smallerops % probably arXiv doesn't have it
]{newpxmath}
%\useosf
%\linespread{1.083}%
%\linespread{1.05}% widely used
\linespread{1.1}% best for text with maths
%% smaller operators for old version of newpxmath
\makeatletter
\def\re@DeclareMathSymbol#1#2#3#4{%
    \let#1=\undefined
    \DeclareMathSymbol{#1}{#2}{#3}{#4}}
%\re@DeclareMathSymbol{\bigsqcupop}{\mathop}{largesymbols}{"46}
%\re@DeclareMathSymbol{\bigodotop}{\mathop}{largesymbols}{"4A}
\re@DeclareMathSymbol{\bigoplusop}{\mathop}{largesymbols}{"4C}
\re@DeclareMathSymbol{\bigotimesop}{\mathop}{largesymbols}{"4E}
\re@DeclareMathSymbol{\sumop}{\mathop}{largesymbols}{"50}
\re@DeclareMathSymbol{\prodop}{\mathop}{largesymbols}{"51}
\re@DeclareMathSymbol{\bigcupop}{\mathop}{largesymbols}{"53}
\re@DeclareMathSymbol{\bigcapop}{\mathop}{largesymbols}{"54}
%\re@DeclareMathSymbol{\biguplusop}{\mathop}{largesymbols}{"55}
\re@DeclareMathSymbol{\bigwedgeop}{\mathop}{largesymbols}{"56}
\re@DeclareMathSymbol{\bigveeop}{\mathop}{largesymbols}{"57}
%\re@DeclareMathSymbol{\bigcupdotop}{\mathop}{largesymbols}{"DF}
%\re@DeclareMathSymbol{\bigcapplusop}{\mathop}{largesymbolsPXA}{"00}
%\re@DeclareMathSymbol{\bigsqcupplusop}{\mathop}{largesymbolsPXA}{"02}
%\re@DeclareMathSymbol{\bigsqcapplusop}{\mathop}{largesymbolsPXA}{"04}
%\re@DeclareMathSymbol{\bigsqcapop}{\mathop}{largesymbolsPXA}{"06}
\re@DeclareMathSymbol{\bigtimesop}{\mathop}{largesymbolsPXA}{"10}
%\re@DeclareMathSymbol{\coprodop}{\mathop}{largesymbols}{"60}
%\re@DeclareMathSymbol{\varprod}{\mathop}{largesymbolsPXA}{16}
\makeatother
%%
%% With euler font cursive for Greek letters - the [1] means 100% scaling
\DeclareFontFamily{U}{egreek}{\skewchar\font'177}%
\DeclareFontShape{U}{egreek}{m}{n}{<-6>s*[1]eurm5 <6-8>s*[1]eurm7 <8->s*[1]eurm10}{}%
\DeclareFontShape{U}{egreek}{m}{it}{<->s*[1]eurmo10}{}%
\DeclareFontShape{U}{egreek}{b}{n}{<-6>s*[1]eurb5 <6-8>s*[1]eurb7 <8->s*[1]eurb10}{}%
\DeclareFontShape{U}{egreek}{b}{it}{<->s*[1]eurbo10}{}%
\DeclareSymbolFont{egreeki}{U}{egreek}{m}{it}%
\SetSymbolFont{egreeki}{bold}{U}{egreek}{b}{it}% from the amsfonts package
\DeclareSymbolFont{egreekr}{U}{egreek}{m}{n}%
\SetSymbolFont{egreekr}{bold}{U}{egreek}{b}{n}% from the amsfonts package
% Take also \sum, \prod, \coprod symbols from Euler fonts
\DeclareFontFamily{U}{egreekx}{\skewchar\font'177}
\DeclareFontShape{U}{egreekx}{m}{n}{%
       <-7.5>s*[0.9]euex7%
    <7.5-8.5>s*[0.9]euex8%
    <8.5-9.5>s*[0.9]euex9%
    <9.5->s*[0.9]euex10%
}{}
\DeclareSymbolFont{egreekx}{U}{egreekx}{m}{n}
\DeclareMathSymbol{\sumop}{\mathop}{egreekx}{"50}
\DeclareMathSymbol{\prodop}{\mathop}{egreekx}{"51}
\DeclareMathSymbol{\coprodop}{\mathop}{egreekx}{"60}
\makeatletter
\def\sum{\DOTSI\sumop\slimits@}
\def\prod{\DOTSI\prodop\slimits@}
\def\coprod{\DOTSI\coprodop\slimits@}
\makeatother
\input{definegreek.tex}% Greek letters not usually given in LaTeX.

%\usepackage%[scaled=0.9]%
%{classico}%  Optima as sans-serif font
\renewcommand\sfdefault{uop}
\DeclareMathAlphabet{\mathsf}  {T1}{\sfdefault}{m}{sl}
\SetMathAlphabet{\mathsf}{bold}{T1}{\sfdefault}{b}{sl}
%\newcommand*{\mathte}[1]{\textbf{\textit{\textsf{#1}}}}
% Upright sans-serif math alphabet
% \DeclareMathAlphabet{\mathsu}  {T1}{\sfdefault}{m}{n}
% \SetMathAlphabet{\mathsu}{bold}{T1}{\sfdefault}{b}{n}

% DejaVu Mono as typewriter text
\usepackage[scaled=0.84]{DejaVuSansMono}

\usepackage{mathdots}

\usepackage[usenames]{xcolor}
% Tol (2012) colour-blind-, print-, screen-friendly colours, alternative scheme; Munsell terminology
\definecolor{mypurpleblue}{RGB}{68,119,170}
\definecolor{myblue}{RGB}{102,204,238}
\definecolor{mygreen}{RGB}{34,136,51}
\definecolor{myyellow}{RGB}{204,187,68}
\definecolor{myred}{RGB}{238,102,119}
\definecolor{myredpurple}{RGB}{170,51,119}
\definecolor{mygrey}{RGB}{187,187,187}
% Tol (2012) colour-blind-, print-, screen-friendly colours; Munsell terminology
% \definecolor{lbpurple}{RGB}{51,34,136}
% \definecolor{lblue}{RGB}{136,204,238}
% \definecolor{lbgreen}{RGB}{68,170,153}
% \definecolor{lgreen}{RGB}{17,119,51}
% \definecolor{lgyellow}{RGB}{153,153,51}
% \definecolor{lyellow}{RGB}{221,204,119}
% \definecolor{lred}{RGB}{204,102,119}
% \definecolor{lpred}{RGB}{136,34,85}
% \definecolor{lrpurple}{RGB}{170,68,153}
\definecolor{lgrey}{RGB}{221,221,221}
%\newcommand*\mycolourbox[1]{%
%\colorbox{mygrey}{\hspace{1em}#1\hspace{1em}}}
\colorlet{shadecolor}{lgrey}

\usepackage{bm}

\usepackage{microtype}

\usepackage[backend=biber,mcite,%subentry,
citestyle=authoryear-comp,bibstyle=pglpm-authoryear,autopunct=false,sorting=ny,sortcites=false,natbib=false,maxcitenames=2,maxbibnames=8,minbibnames=8,giveninits=true,uniquename=false,uniquelist=false,maxalphanames=1,block=space,hyperref=true,defernumbers=false,useprefix=true,sortupper=false,language=british,parentracker=false]{biblatex}
\DeclareSortingTemplate{ny}{\sort{\field{sortname}\field{author}\field{editor}}\sort{\field{year}}}
\iffalse\makeatletter%%% replace parenthesis with brackets
\newrobustcmd*{\parentexttrack}[1]{%
  \begingroup
  \blx@blxinit
  \blx@setsfcodes
  \blx@bibopenparen#1\blx@bibcloseparen
  \endgroup}
\AtEveryCite{%
  \let\parentext=\parentexttrack%
  \let\bibopenparen=\bibopenbracket%
  \let\bibcloseparen=\bibclosebracket}
\makeatother\fi
\DefineBibliographyExtras{british}{\def\finalandcomma{\addcomma}}
\renewcommand*{\finalnamedelim}{\addspace\amp\space}
% \renewcommand*{\finalnamedelim}{\addcomma\space}
\renewcommand*{\textcitedelim}{\addcomma\space}
% \setcounter{biburlnumpenalty}{1}
% \setcounter{biburlucpenalty}{0}
% \setcounter{biburllcpenalty}{1}
\DeclareDelimFormat{multicitedelim}{\addsemicolon\addspace\space}
\DeclareDelimFormat{compcitedelim}{\addsemicolon\addspace\space}
\DeclareDelimFormat{postnotedelim}{\addspace}
\ifarxiv\else\addbibresource{portamanabib.bib}\fi
\renewcommand{\bibfont}{\footnotesize}
%\appto{\citesetup}{\footnotesize}% smaller font for citations
\defbibheading{bibliography}[\bibname]{\section*{#1}\addcontentsline{toc}{section}{#1}%\markboth{#1}{#1}
}
\newcommand*{\citep}{\footcites}
%\newcommand*{\citey}{\footcites}%{\parencites*}
\newcommand*{\citey}{\parencites*}
\newcommand*{\ibid}{\unspace\addtocounter{footnote}{-1}\footnotemark{}}
%\renewcommand*{\cite}{\parencite}
%\renewcommand*{\cites}{\parencites}
\providecommand{\href}[2]{#2}
\providecommand{\eprint}[2]{\texttt{\href{#1}{#2}}}
\newcommand*{\amp}{\&}
% \newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
% }
\newcommand*{\citein}[2][]{\textnormal{\textcite[#1]{#2}}%\addtocategory{extras}{#2}
}
\newcommand*{\citebi}[2][]{\textcite[#1]{#2}%\addtocategory{extras}{#2}
}
\newcommand*{\subtitleproc}[1]{}
\newcommand*{\chapb}{ch.}
%
\DeclareUrlCommand\doiurl{\urlstyle{rm}}
\newcommand*{\arxiveprint}[1]{%
\texttt{arXiv:\urlalt{https://arxiv.org/abs/#1}{\doiurl{#1}}}%
}
\newcommand*{\mparceprint}[1]{%
\texttt{mp\_arc:\urlalt{http://www.ma.utexas.edu/mp_arc-bin/mpa?yn=#1}{\doiurl{#1}}}%
}
\newcommand*{\haleprint}[1]{%
\texttt{HAL:\urlalt{https://hal.archives-ouvertes.fr/#1}{\doiurrl{#1}}}%
}
\newcommand*{\philscieprint}[1]{%
\texttt{PhilSci:\urlalt{http://philsci-archive.pitt.edu/archive/#1}{\doiurl{#1}}}%
}
\newcommand*{\doi}[1]{%
\href{https://doi.org/#1}{\textsc{doi}:\allowbreak\texttt{\doiurl{#1}}}%
}
\newcommand*{\biorxiveprint}[1]{%
bioRxiv \doi{10.1101/#1}%
}
\newcommand*{\osfeprint}[1]{%
Open Science Framework \doi{10.31219/osf.io/#1}%
}

\usepackage{graphicx}

%\usepackage{wrapfig}

%\usepackage{tikz-cd}

\PassOptionsToPackage{hyphens}{url}\usepackage[hypertexnames=false,pdfencoding=unicode,psdextra]{hyperref}

\usepackage[depth=4]{bookmark}
\hypersetup{colorlinks=true,bookmarksnumbered,pdfborder={0 0 0.25},citebordercolor={0.2667 0.4667 0.6667},citecolor=mypurpleblue,linkbordercolor={0.6667 0.2 0.4667},linkcolor=myredpurple,urlbordercolor={0.1333 0.5333 0.2},urlcolor=mygreen,breaklinks=true,pdftitle={\pdftitle},pdfauthor={\pdfauthor}}
% \usepackage[vertfit=local]{breakurl}% only for arXiv
\providecommand*{\urlalt}{\href}

\usepackage[british]{datetime2}
\DTMnewdatestyle{mydate}%
{% definitions
\renewcommand*{\DTMdisplaydate}[4]{%
\number##3\ \DTMenglishmonthname{##2} ##1}%
\renewcommand*{\DTMDisplaydate}{\DTMdisplaydate}%
}
\DTMsetdatestyle{mydate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Layout. I do not know on which kind of paper the reader will print the
%%% paper on (A4? letter? one-sided? double-sided?). So I choose A5, which
%%% provides a good layout for reading on screen and save paper if printed
%%% two pages per sheet. Average length line is 66 characters and page
%%% numbers are centred.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ifafour\setstocksize{297mm}{210mm}%{*}% A4
\else\setstocksize{210mm}{5.5in}%{*}% 210x139.7
\fi
\settrimmedsize{\stockheight}{\stockwidth}{*}
\setlxvchars[\normalfont] %313.3632pt for a 66-characters line
\setxlvchars[\normalfont]
% \setlength{\trimtop}{0pt}
% \setlength{\trimedge}{\stockwidth}
% \addtolength{\trimedge}{-\paperwidth}
%\settrims{0pt}{0pt}
% The length of the normalsize alphabet is 133.05988pt - 10 pt = 26.1408pc
% The length of the normalsize alphabet is 159.6719pt - 12pt = 30.3586pc
% Bringhurst gives 32pc as boundary optimal with 69 ch per line
% The length of the normalsize alphabet is 191.60612pt - 14pt = 35.8634pc
\ifafour\settypeblocksize{*}{32pc}{1.618} % A4
%\setulmargins{*}{*}{1.667}%gives 5/3 margins % 2 or 1.667
\else\settypeblocksize{*}{26pc}{1.618}% nearer to a 66-line newpx and preserves GR
\fi
\setulmargins{*}{*}{1}%gives equal margins
\setlrmargins{*}{*}{*}
\setheadfoot{\onelineskip}{2.5\onelineskip}
\setheaderspaces{*}{2\onelineskip}{*}
\setmarginnotes{2ex}{10mm}{0pt}
\checkandfixthelayout[nearest]
%%% End layout
%% this fixes missing white spaces
%\pdfmapline{+dummy-space <dummy-space.pfb}
%\pdfinterwordspaceon% seems to add a white margin to Sumatrapdf

%%% Sectioning
\newcommand*{\asudedication}[1]{%
{\par\centering\textit{#1}\par}}
\newenvironment{acknowledgements}{\section*{Thanks}\addcontentsline{toc}{section}{Thanks}}{\par}
\makeatletter\renewcommand{\appendix}{\par
  \bigskip{\centering
   \interlinepenalty \@M
   \normalfont
   \printchaptertitle{\sffamily\appendixpagename}\par}
  \setcounter{section}{0}%
  \gdef\@chapapp{\appendixname}%
  \gdef\thesection{\@Alph\c@section}%
  \anappendixtrue}\makeatother
\counterwithout{section}{chapter}
\setsecnumformat{\upshape\csname the#1\endcsname\quad}
\setsecheadstyle{\large\bfseries\sffamily%
\centering}
\setsubsecheadstyle{\bfseries\sffamily%
\raggedright}
%\setbeforesecskip{-1.5ex plus 1ex minus .2ex}% plus 1ex minus .2ex}
%\setaftersecskip{1.3ex plus .2ex }% plus 1ex minus .2ex}
%\setsubsubsecheadstyle{\bfseries\sffamily\slshape\raggedright}
%\setbeforesubsecskip{1.25ex plus 1ex minus .2ex }% plus 1ex minus .2ex}
%\setaftersubsecskip{-1em}%{-0.5ex plus .2ex}% plus 1ex minus .2ex}
\setsubsecindent{0pt}%0ex plus 1ex minus .2ex}
\setparaheadstyle{\bfseries\sffamily%
\raggedright}
\setcounter{secnumdepth}{2}
\setlength{\headwidth}{\textwidth}
\newcommand{\addchap}[1]{\chapter*[#1]{#1}\addcontentsline{toc}{chapter}{#1}}
\newcommand{\addsec}[1]{\section*{#1}\addcontentsline{toc}{section}{#1}}
\newcommand{\addsubsec}[1]{\subsection*{#1}\addcontentsline{toc}{subsection}{#1}}
\newcommand{\addpara}[1]{\paragraph*{#1.}\addcontentsline{toc}{subsubsection}{#1}}
\newcommand{\addparap}[1]{\paragraph*{#1}\addcontentsline{toc}{subsubsection}{#1}}

%%% Headers, footers, pagestyle
\copypagestyle{manaart}{plain}
\makeheadrule{manaart}{\headwidth}{0.5\normalrulethickness}
\makeoddhead{manaart}{%
{\footnotesize%\sffamily%
\scshape\headauthor}}{}{{\footnotesize\sffamily%
\headtitle}}
\makeoddfoot{manaart}{}{\thepage}{}
\newcommand*\autanet{\includegraphics[height=\heightof{M}]{autanet.pdf}}
\definecolor{mygray}{gray}{0.333}
\iftypodisclaim%
\ifafour\newcommand\addprintnote{\begin{picture}(0,0)%
\put(245,149){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}% A4
\else\newcommand\addprintnote{\begin{picture}(0,0)%
\put(176,112){\makebox(0,0){\rotatebox{90}{\tiny\color{mygray}\textsf{This
            document is designed for screen reading and
            two-up printing on A4 or Letter paper}}}}%
\end{picture}}\fi%afourtrue
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}\addprintnote}{}
\else
\makeoddfoot{plain}{}{\makebox[0pt]{\thepage}}{}
\fi%typodisclaimtrue
\makeoddhead{plain}{\scriptsize\reporthead}{}{}
% \copypagestyle{manainitial}{plain}
% \makeheadrule{manainitial}{\headwidth}{0.5\normalrulethickness}
% \makeoddhead{manainitial}{%
% \footnotesize\sffamily%
% \scshape\headauthor}{}{\footnotesize\sffamily%
% \headtitle}
% \makeoddfoot{manaart}{}{\thepage}{}

\pagestyle{manaart}

\setlength{\droptitle}{-3.9\onelineskip}
\pretitle{\begin{center}\LARGE\sffamily%
\bfseries}
\posttitle{\bigskip\end{center}}

\makeatletter\newcommand*{\atf}{\includegraphics[totalheight=\heightof{@}]{atblack.png}}\makeatother
\providecommand{\affiliation}[1]{\textsl{\textsf{\footnotesize #1}}}
\providecommand{\epost}[1]{\texttt{\footnotesize\textless#1\textgreater}}
\providecommand{\email}[2]{\href{mailto:#1ZZ@#2 ((remove ZZ))}{#1\protect\atf#2}}
%\providecommand{\email}[2]{\href{mailto:#1@#2}{#1@#2}}

\preauthor{\vspace{-0.5\baselineskip}\begin{center}
\normalsize\sffamily%
\lineskip  0.5em}
\postauthor{\par\end{center}}
\predate{\DTMsetdatestyle{mydate}\begin{center}\footnotesize}
\postdate{\end{center}\vspace{-\medskipamount}}

\setfloatadjustment{figure}{\footnotesize}
\captiondelim{\quad}
\captionnamefont{\footnotesize\sffamily%
}
\captiontitlefont{\footnotesize}
%\firmlists*
\midsloppy
% handling orphan/widow lines, memman.pdf
% \clubpenalty=10000
% \widowpenalty=10000
% \raggedbottom
% Downes, memman.pdf
\clubpenalty=9996
\widowpenalty=9999
\brokenpenalty=4991
\predisplaypenalty=10000
\postdisplaypenalty=1549
\displaywidowpenalty=1602
\raggedbottom

\paragraphfootnotes
\setlength{\footmarkwidth}{2ex}
% \threecolumnfootnotes
%\setlength{\footmarksep}{0em}
\footmarkstyle{\textsuperscript{%\color{myred}
\scriptsize\bfseries#1}~}
%\footmarkstyle{\textsuperscript{\color{myred}\scriptsize\bfseries#1}~}
%\footmarkstyle{\textsuperscript{[#1]}~}

\selectlanguage{british}\frenchspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Paper's details
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{\propertitle}
\author{%
\hspace*{\stretch{1}}%
%% uncomment if additional authors present
% \parbox{0.5\linewidth}%\makebox[0pt][c]%
% {\protect\centering ***\\%
% \footnotesize\epost{\email{***}{***}}}%
% \hspace*{\stretch{1}}%
\parbox{1\linewidth}%\makebox[0pt][c]%
{\protect\centering Luca  \href{https://orcid.org/0000-0002-6070-0784}{\protect\includegraphics[scale=0.16]{orcid_32x32.png}}\quad
\footnotesize% Western Norway University of Applied Sciences\quad%
\epost{\email{pgl}{portamana.org}}}%
% Mohn Medical Imaging and Visualization Centre, Dept of Computer science, Electrical Engineering and Mathematical Sciences, Western Norway University of Applied Sciences, Bergen, Norway
%% uncomment if additional authors present
% \hspace*{\stretch{1}}%
% \parbox{0.5\linewidth}%\makebox[0pt][c]%
% {\protect\centering ***\\%
% \footnotesize\epost{\email{***}{***}}}%
\hspace*{\stretch{1}}%
}

%\date{Draft of \today\ (first drafted \firstdraft)}
\date{\firstpublished; updated \updated}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Macros @@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Common ones - uncomment as needed
%\providecommand{\nequiv}{\not\equiv}
%\providecommand{\coloneqq}{\mathrel{\mathop:}=}
%\providecommand{\eqqcolon}{=\mathrel{\mathop:}}
%\providecommand{\varprod}{\prod}
\newcommand*{\de}{\partialup}%partial diff
\newcommand*{\pu}{\piup}%constant pi
\newcommand*{\delt}{\deltaup}%Kronecker, Dirac
%\newcommand*{\eps}{\varepsilonup}%Levi-Civita, Heaviside
%\newcommand*{\riem}{\zetaup}%Riemann zeta
%\providecommand{\degree}{\textdegree}% degree
%\newcommand*{\celsius}{\textcelsius}% degree Celsius
%\newcommand*{\micro}{\textmu}% degree Celsius
\newcommand*{\I}{\mathrm{i}}%imaginary unit
\newcommand*{\e}{\mathrm{e}}%Neper
\newcommand*{\di}{\mathrm{d}}%differential
%\newcommand*{\Di}{\mathrm{D}}%capital differential
%\newcommand*{\planckc}{\hslash}
%\newcommand*{\avogn}{N_{\textrm{A}}}
%\newcommand*{\NN}{\bm{\mathrm{N}}}
%\newcommand*{\ZZ}{\bm{\mathrm{Z}}}
%\newcommand*{\QQ}{\bm{\mathrm{Q}}}
\newcommand*{\RR}{\bm{\mathrm{R}}}
%\newcommand*{\CC}{\bm{\mathrm{C}}}
%\newcommand*{\nabl}{\bm{\nabla}}%nabla
%\DeclareMathOperator{\lb}{lb}%base 2 log
%\DeclareMathOperator{\tr}{tr}%trace
%\DeclareMathOperator{\card}{card}%cardinality
%\DeclareMathOperator{\im}{Im}%im part
%\DeclareMathOperator{\re}{Re}%re part
%\DeclareMathOperator{\sgn}{sgn}%signum
%\DeclareMathOperator{\ent}{ent}%integer less or equal to
%\DeclareMathOperator{\Ord}{O}%same order as
%\DeclareMathOperator{\ord}{o}%lower order than
%\newcommand*{\incr}{\triangle}%finite increment
\newcommand*{\defd}{\coloneqq}
\newcommand*{\defs}{\eqqcolon}
%\newcommand*{\Land}{\bigwedge}
%\newcommand*{\Lor}{\bigvee}
%\newcommand*{\lland}{\DOTSB\;\land\;}
%\newcommand*{\llor}{\DOTSB\;\lor\;}
%\newcommand*{\limplies}{\mathbin{\Rightarrow}}%implies
\newcommand*{\suchthat}{\mid}%{\mathpunct{|}}%such that (eg in sets)
%\newcommand*{\with}{\colon}%with (list of indices)
%\newcommand*{\mul}{\times}%multiplication
%\newcommand*{\inn}{\cdot}%inner product
%\newcommand*{\dotv}{\mathord{\,\cdot\,}}%variable place
%\newcommand*{\comp}{\circ}%composition of functions
%\newcommand*{\con}{\mathbin{:}}%scal prod of tensors
%\newcommand*{\equi}{\sim}%equivalent to
\renewcommand*{\asymp}{\simeq}%equivalent to
%\newcommand*{\corr}{\mathrel{\hat{=}}}%corresponds to
%\providecommand{\varparallel}{\ensuremath{\mathbin{/\mkern-7mu/}}}%parallel (tentative symbol)
\renewcommand*{\le}{\leqslant}%less or equal
\renewcommand*{\ge}{\geqslant}%greater or equal
\DeclarePairedDelimiter\clcl{[}{]}
\DeclarePairedDelimiter\clop{[}{[}
%\DeclarePairedDelimiter\opcl{]}{]}
%\DeclarePairedDelimiter\opop{]}{[}
\DeclarePairedDelimiter\abs{\lvert}{\rvert}
%\DeclarePairedDelimiter\norm{\lVert}{\rVert}
\DeclarePairedDelimiter\set{\{}{\}}%}
%\DeclareMathOperator{\pr}{P}%probability
\newcommand*{\p}{\mathrm{p}}%probability
\renewcommand*{\P}{\mathrm{P}}%probability
%\newcommand*{\E}{\mathrm{E}}
%\renewcommand*{\|}{\nonscript\,\vert\nonscript\;\mathopen{}}
\renewcommand*{\|}[1][]{\nonscript\:#1\vert\nonscript\:\mathopen{}}
%\DeclarePairedDelimiterX{\cp}[2]{(}{)}{#1\nonscript\:\delimsize\vert\nonscript\:\mathopen{}#2}
%\DeclarePairedDelimiterX{\ct}[2]{[}{]}{#1\nonscript\;\delimsize\vert\nonscript\:\mathopen{}#2}
%\DeclarePairedDelimiterX{\cs}[2]{\{}{\}}{#1\nonscript\:\delimsize\vert\nonscript\:\mathopen{}#2}
%\newcommand*{\+}{\lor}
%\renewcommand{\*}{\land}
%% symbol = for equality statements within probabilities
%% from https://tex.stackexchange.com/a/484142/97039
\newcommand*{\eq}{\mathrel{\!=\!}}
\let\texteq\=
\renewcommand*{\=}{\TextOrMath\texteq\eq}
%%
\newcommand*{\sect}{\S}% Sect.~
\newcommand*{\sects}{\S\S}% Sect.~
\newcommand*{\chap}{ch.}%
\newcommand*{\chaps}{chs}%
\newcommand*{\bref}{ref.}%
\newcommand*{\brefs}{refs}%
%\newcommand*{\fn}{fn}%
\newcommand*{\eqn}{eq.}%
\newcommand*{\eqns}{eqs}%
\newcommand*{\fig}{fig.}%
\newcommand*{\figs}{figs}%
\newcommand*{\vs}{{vs}}
\newcommand*{\eg}{{e.g.}}
\newcommand*{\etc}{{etc.}}
\newcommand*{\ie}{{i.e.}}
%\newcommand*{\ca}{{c.}}
\newcommand*{\foll}{{ff.}}
%\newcommand*{\viz}{{viz}}
\newcommand*{\cf}{{cf.}}
%\newcommand*{\Cf}{{Cf.}}
%\newcommand*{\vd}{{v.}}
\newcommand*{\etal}{{et al.}}
%\newcommand*{\etsim}{{et sim.}}
%\newcommand*{\ibid}{{ibid.}}
%\newcommand*{\sic}{{sic}}
%\newcommand*{\id}{\mathte{I}}%id matrix
%\newcommand*{\nbd}{\nobreakdash}%
%\newcommand*{\bd}{\hspace{0pt}}%
%\def\hy{-\penalty0\hskip0pt\relax}
%\newcommand*{\labelbis}[1]{\tag*{(\ref{#1})$_\text{r}$}}
%\newcommand*{\mathbox}[2][.8]{\parbox[t]{#1\columnwidth}{#2}}
%\newcommand*{\zerob}[1]{\makebox[0pt][l]{#1}}
\newcommand*{\tprod}{\mathop{\textstyle\prod}\nolimits}
\newcommand*{\tsum}{\mathop{\textstyle\sum}\nolimits}
%\newcommand*{\tint}{\begingroup\textstyle\int\endgroup\nolimits}
%\newcommand*{\tland}{\mathop{\textstyle\bigwedge}\nolimits}
%\newcommand*{\tlor}{\mathop{\textstyle\bigvee}\nolimits}
%\newcommand*{\sprod}{\mathop{\textstyle\prod}}
%\newcommand*{\ssum}{\mathop{\textstyle\sum}}
%\newcommand*{\sint}{\begingroup\textstyle\int\endgroup}
%\newcommand*{\sland}{\mathop{\textstyle\bigwedge}}
%\newcommand*{\slor}{\mathop{\textstyle\bigvee}}
%\newcommand*{\T}{^\transp}%transpose
%%\newcommand*{\QEM}%{\textnormal{$\Box$}}%{\ding{167}}
%\newcommand*{\qem}{\leavevmode\unskip\penalty9999 \hbox{}\nobreak\hfill
%\quad\hbox{\QEM}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Custom macros for this file @@@
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{notecolour}{RGB}{68,170,153}
%\newcommand*{\puzzle}{\maltese}
\newcommand*{\puzzle}{{\fontencoding{U}\fontfamily{fontawesometwo}\selectfont\symbol{225}}}
\newcommand*{\wrench}{{\fontencoding{U}\fontfamily{fontawesomethree}\selectfont\symbol{114}}}
\newcommand*{\pencil}{{\fontencoding{U}\fontfamily{fontawesometwo}\selectfont\symbol{210}}}
\newcommand{\mynote}[1]{ {\color{notecolour}#1}}

\newcommand*{\widebar}[1]{{\mkern1.5mu\skew{2}\overline{\mkern-1.5mu#1\mkern-1.5mu}\mkern 1.5mu}}

% \newcommand{\explanation}[4][t]{%\setlength{\tabcolsep}{-1ex}
% %\smash{
% \begin{tabular}[#1]{c}#2\\[0.5\jot]\rule{1pt}{#3}\\#4\end{tabular}}%}
% \newcommand*{\ptext}[1]{\text{\small #1}}
%\DeclareMathOperator*{\argsup}{arg\,sup}
% from https://tex.stackexchange.com/a/424252/97039
\makeatletter
\newcommand*{\q}{}% Check if undefined
\DeclareRobustCommand*{\q}{%
  \mathord{\mathpalette\bigcdot@{}}% changed mathbin to mathord
}
\newcommand*{\bigcdot@scalefactor}{0.7}
\newcommand*{\bigcdot@widthfactor}{1.5}
\newcommand*{\bigcdot@}[2]{%
  % #1: math style
  % #2: unused
  \sbox0{$#1\vcenter{}$}% math axis
  \sbox2{$#1\cdot\m@th$}%
  \hbox to \bigcdot@widthfactor\wd2{%
    \hfil
    \raise\ht0\hbox{%
      \scalebox{\bigcdot@scalefactor}{%
        \lower\ht0\hbox{$#1\bullet\m@th$}%
      }%
    }%
    \hfil
  }%
}
\makeatother
%\newcommand*{\q}{\bullet}
\DeclareMathOperator*{\logit}{logit}
\newcommand*{\dob}{degree of belief}
\newcommand*{\dobs}{degrees of belief}
\newcommand*{\rms}{root-mean-square}
\newcommand*{\rmsd}{\textsc{rmsd}}
\newcommand*{\ro}{r}
\newcommand*{\xo}{x}
\newcommand*{\rd}{\bar{r}}
\newcommand*{\xd}{\bar{x}}
\newcommand*{\yF}{\bm{F}}
\newcommand*{\yFr}{\yF_{\bm{\mid}r}}
\newcommand*{\yFrx}{\yF_{\bm{r\mid x}}}
\newcommand*{\yFxr}{\yF_{\bm{x\mid r}}}
%%% Custom macros end @@@

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Beginning of document
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\firmlists
\begin{document}
\captiondelim{\quad}\captionnamefont{\footnotesize}\captiontitlefont{\footnotesize}
\selectlanguage{british}\frenchspacing
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Abstract
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\abstractrunin
\abslabeldelim{}
\renewcommand*{\abstractname}{}
\setlength{\absleftindent}{0pt}
\setlength{\absrightindent}{0pt}
\setlength{\abstitleskip}{-\absparindent}
\begin{abstract}\labelsep 0pt%
  \noindent %***
% \\\noindent\emph{\footnotesize Note: Dear Reader
%     \amp\ Peer, this manuscript is being peer-reviewed by you. Thank you.}
% \par%\\[\jot]
% \noindent
% {\footnotesize PACS: ***}\qquad%
% {\footnotesize MSC: ***}%
%\qquad{\footnotesize Keywords: ***}
\end{abstract}
\selectlanguage{british}\frenchspacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Epigraph
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \asudedication{\small ***}
% \vspace{\bigskipamount}
% \setlength{\epigraphwidth}{.7\columnwidth}
% %\epigraphposition{flushright}
% \epigraphtextposition{flushright}
% %\epigraphsourceposition{flushright}
% \epigraphfontsize{\footnotesize}
% \setlength{\epigraphrule}{0pt}
% %\setlength{\beforeepigraphskip}{0pt}
% %\setlength{\afterepigraphskip}{0pt}
% \epigraph{\emph{text}}{source}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% BEGINNING OF MAIN TEXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mynote{\wrench\ \textsc{to be completely rewritten: new approach is based on
    \textcite{dunsonetal2011} and \textcite{nimble2016_r2021}}}

\bigskip

\mynote{\footnotesize\puzzle\ Note: the probability-theoretic derivations and
  explanations below are very concise and likely not fully
  comprehensible. I'll add some sections with clearer explanations and
  fuller derivations later on.}

\section{Overview}
\label{sec:overview}

We consider a quantity, the Root-Mean-Squared Distance, denoted by $r$, and
a set of quantities called \enquote{features}, denoted by $x$. A
value of \rmsd\ $r$
and values for the  features $x$ can be associated with each target-ligand
pair, which I'll call a \enquote{datapoint}.

Our problem is to infer the value $\ro$ of a new datapoint, given:
\begin{itemize}
\item features $\xo$ for the new datapoint;
\item known pairs
  $(\rd,\xd) \defd \bigl((r_{1}, x_{1}), (r_{2}, x_{1}), \dotsc, (r_{N},
  x_{N})\bigr)$ for other $N$ datapoints, called the \enquote{training
    set};
\item additional facts and hypotheses $H$ (unavoidable since we are making an
extrapolation).
\end{itemize}

Our uncertainty about $\ro$ is expressed by the probability
\begin{equation}
  \label{eq:main_prob}
  \p(\ro \| \xo,\ \rd,\xd,\ H)
\end{equation}
which we want to quantify using the probability calculus.

In the next section we will examine several possible underlying hypotheses
and calculate the probabilities they lead to. In \mynote{\wrench\ \sect***}
we will examine the consequences of the different hypotheses and their
ensuing probabilities on a test dataset.


\section{Underlying hypotheses}
\label{sec:hypotheses}

Quantifying the probability~\eqref{eq:main_prob} requires two main
assessments:
\begin{itemize}
\item The probability distribution for the long-run relative frequencies of
  \rmsd\ and feature values that the training set would have if,
  hypothetically, it were augmented indefinitely.
\item What kind of relevance the training set has for our inference about
  the new datapoint.
\end{itemize}
We discuss these assessments in the next two sections.
Section~\ref{sec:p_final_inference} combines them into final probability
formulae.


\subsection{Probability for long-run frequencies}
\label{sec:prob_longrun}

Let us imagine to extend the size of the training set indefinitely, in such
a way that our probability assignments for any subset of it would be
exchangeable (this condition is more general than an \enquote{i.i.d.} one).
We could then measure the joint long-run relative frequencies $F_{r\,x}$
for all values $r$ and $x$ in such extended training
set\footnote{\enquote{But this \emph{long run} is a misleading guide to
    current affairs. \emph{In the long run} we are all dead.}
  \citep[\sect~3.I, p.~65]{keynes1923_r2013}}, as well as the marginal
relative frequencies $F_{r\q}$ for $r$, and $F_{\q x}$ for $x$; and the
conditional relative frequencies $F_{r\mid x}$ for $r$ given $x$, and
$F_{x \mid r}$ for $x$ given $r$. These frequencies are related by
\begin{equation}
  \label{eq:frequencies_relations}
  F_{r\q}=\sum_{x} F_{r\,x}\ , \quad
  F_{\q x}=\sum_{r} F_{r\,x}\ ,\quad
  F_{r\mid x}=\frac{F_{r\,x}}{F_{\q x}}\ , \quad
  F_{x\mid r}=\frac{F_{r\,x}}{F_{r\q}}\ .
\end{equation}
The collection of joint frequencies $\bigl(F_{r\,x}\bigr)_{r\,x}$ is
denoted by $\yF$. For later use we denote by $\yFr$ the collection of
conditional frequencies $\bigl(F_{x\mid r}\bigr)_{x}$, for each value of
$r$. Note that from $\yF$ we can obtain $\yFr$ and $F_{r\q}$ for each $r$,
and vice versa.

\medskip

We need to specify a probability density
\begin{equation}\label{eq:prior_F}
\p(\yF \| H)\ \di\yF
\end{equation}
representing our belief and prior information as to what the long-run
frequencies could be, before our observation of the training set. After the
observation of the training set this probability is updated to
\begin{equation}
  \label{eq:post_prob_F}
  \p(\yF\|\rd,\xd, H)\,\di\yF =
  \frac{\p(\yF\| H)\
    \prod_{i=1}^{N} F_{r_{i}\,x_{i}}
  }{\int \p(\yF\| H)\
    \prod_{i=1}^{N} F_{r_{i}\,x_{i}}
              \ \di\yF \ .
  }\ \di\yF
  % \frac{\displaystyle \p(\yF\| H)\
  %   \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}
  % }{\int \p(\yF\| H)\
  %      \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}
  %             \ \di\yF
  % }\ \di\yF
\end{equation}
The product $\prod_{i=1}^{N} F_{r_{i}\,x_{i}} = \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}$,
where $f_{r\,x}$ is the joint frequency of values $r$ and $x$ observed in
the training set.


The choice of density~\eqref{eq:prior_F} is constrained by analytic
convenience and computational costs -- such a density is defined over an
infinite-dimensional manifold. Our specific choices and their motivations are
discussed more in detail in \mynote{\wrench appendix***}. Here we briefly
discuss two main possibilities:
\begin{enumerate}[label=\Roman*.]
\item\label{item:p_F_unfactorizable} An unfactorizable density:
    \begin{equation}
    \label{eq:p_F_unfactorizable}
    \p(\yF\| H_{\textrm{I}})\ \di\yF
    % \ne \prod_{r} \p(\yFr \| H_{\textrm{II}})\
    % \p(F_{r\q} \| H_{\textrm{II}})\ \di\yFr\ \di F_{r\q}
    \ .
  \end{equation}

\item\label{item:p_F_factorizable} A density factorizable in the
  conditional frequencies given $r$:
  \begin{equation}
    \label{eq:p_F_factorizable}
    \p(\yF\| H_{\textrm{II}})\ \di\yF =
    \prod_{r} \p(\yFr \| H_{\textrm{II}})\
    \p(F_{r\q} \| H_{\textrm{II}})\ \di\yFr\ \di F_{r\q} \ .
  \end{equation}
  It represents the assumption that the conditional frequencies $\yFr$ are
  completely uninformative about the marginal frequencies $(F_{r\q})_{r}$,
  and vice versa.
\end{enumerate}
The first possibility will be used with assumption~\ref{item:exch_xr} of
\sect~\ref{sec:hyp_exch}, leading to simplified and computationally cheaper
formulae. The factors $\p(F_{r\q} \| H)\,\di F_{r\q}$ in particular will
become irrelevant. The updated density also factorizes in this case:
  \begin{multline}
  \label{eq:post_prob_F_factorizable}
  \p(\yF\|\rd,\xd, H_{\textrm{II}})\,\di\yF \propto
  \prod_{r} \p(F_{r\q} \| H_{\textrm{II}})\ {F_{r\q}}^{N\,f_{r\q}}\
  \di F_{r\q} \times{}\\
  \prod_{r} \p(\yFr \| H_{\textrm{II}})\
  \Bigl(\prod_{\substack{i=1\\r_{i}=\ro}}^{N} F_{x_{i}\mid \ro}\Bigr)\ \di\yFr \ ,
\end{multline}
where $f_{r\q}$ is the marginal frequency of the $r$ value in the training
set. The marginal densities for the long-run conditional frequencies
$\yFr$, for each $r$, are also found more easily:
  \begin{equation}
  \label{eq:post_prob_Fxcondr}
  \p(\yFr\|\rd,\xd, H_{\textrm{II}})\,\di\yFr \propto
  \p(\yFr \| H_{\textrm{II}})\
  \Bigl(\prod_{\substack{i=1\\r_{i}=\ro}}^{N} F_{x_{i}\mid \ro}\Bigr)\ \di\yFr \ .
\end{equation}

\medskip

The prior and posterior probability densities for the long-run frequencies
cannot be given in closed form. In practice they are represented by means
of a finite number $T$ of typical samples,
$\set{\yF^{(1)}, \yF^{(2)}, \dotsc, \yF^{(T)}}$, obtained by Markov-chain
Monte Carlo methods. Integrations with respect to these densities is
approximated by sums over these representative samples. These
approximations are discussed in \mynote{\wrench\sect***}.


\subsection{Relevance of training set: exchangeability and subpopulations}
\label{sec:hyp_exch}

The relevance of the training set to the new datapoint is usually expressed
mathematically as a form of symmetry or \emph{exchangeability} of the joint
probability of the training set and the new datapoint. Such a symmetry
intuitively tells us if and how the new datapoint could be considered as a
member of the training set if the latter were hypothetically augmented
indefinitely. Then the long-run frequencies considered in the previous
section directly give us probabilities for the new datapoint.

% In terms of
% \enquote{populations}, such symmetries represent different kinds of
% membership of the new datapoint in various subpopulations that can be
% discerned in the training set.

The question of exchangeability is discussed in a brilliant paper by
Lindley \amp\ Novick \citey{lindleyetal1981}, who show with examples its
fundamental importance for making correct inferences. A short intuitive
summary of this notion is given in \mynote{\wrench appendix***}.

We consider three mutually exclusive cases of exchangeability, the first
seeming the most appropriate to our inference:
\begin{enumerate}[label=\roman*.]
\item\label{item:exch_xr}\emph{Exchangeability in features $x$ given \rmsd\
    $r$}. The values of the features of the training set are relevant to
  the inference of those of the new datapoint, given the same \rmsd\ value.
  The \rmsd s of the training set, however, are not relevant for the \rmsd\
  of the new datapoint. This means that the probability for feature value
  $\xo$ in the new datapoint, given \rmsd\ $\ro$, would be equal to the
  long-run conditional frequency $F_{\xo\mid\ro}$ if the latter were known:
  \begin{equation}
    \label{eq:p_xr_F}
    \p(\xo \| \ro,\ \yF, H_{\textrm{i}}) = F_{\xo\mid\ro} \ .
  \end{equation}

\item\label{item:exch_rx}\emph{Exchangeability in \rmsd\ $r$ given features
    $x$}. The \rmsd s of the training set are relevant to the inference of
  the \rmsd\ of the new datapoint, given the same feature values. The
  feature values of the training set, however, are not relevant for the
  inference of the feature values of the new datapoint. This means that the
  probability for a \rmsd\ $\ro$ in the new datapoint, given features
  $\xo$, would be equal to the long-run conditional frequency
  $F_{\ro\mid\xo}$ if the latter were known:
  \begin{equation}
    \label{eq:p_rx_F}
    \p(\ro \| \xo,\ \yF, H_{\textrm{ii}}) = F_{\ro\mid\xo} \ .
  \end{equation}

\item\label{item:exch_full}\emph{Exchangeability in \rmsd\ $r$ and features
    $x$}. Both the \rmsd s and the values of the features of the training
  set are relevant to the inference of those of the new datapoint. This is
  equivalent to the relevance of $r$ given $x$, and of $x$; and to the
  relevance of $x$ given $r$, and of $r$. It also means that the
  probability for a \rmsd\ $\ro$ in the new datapoint, given features
  $\xo$, would be equal to the long-run conditional frequency
  $F_{\ro\mid\xo}$ if the latter were known:
  \begin{equation}
    \label{eq:p_full_F}
    \p(\ro \| \xo,\ \yF, H_{\textrm{iii}}) = F_{\ro\mid\xo} \ .
  \end{equation}
  Owing to the exchangeability in $x$, we also have that our probability
  for $\yF$ is updated by our knowledge of $\xo$ of the new datapoint:
  \begin{equation}
    \label{eq:F_update_xo}
    \p(\yF \| \xo,\ \rd, \xd, H) =
    \frac{F_{\q\xo}\ \p(\yF \| \rd, \xd, H)}{
    \int F_{\q\xo}\ \p(\yF \| \rd, \xd, H)\ \di\yF} \ .
  \end{equation}
\end{enumerate}
% The mathematical representation of the three kinds of exchangeability above
% is given in \mynote{\wrench\sect***} for brevity.

In the first and second case, the features or \rmsd\ of the training set
could be irrelevant because the respective quantity of the new datapoint
could be selected by hand or by some process different from that underlying
the training set. Several authors connect or motivate these different kinds
of relevance with the robustness of \enquote{causal relations} (which can
be direct or indirect, stemming for example from a common cause) in the
directions $\xo \rightsquigarrow \ro$ or $\ro \rightsquigarrow \xo$
\parencites[\cf][\sects~2.1.2, 2.2.5]{pearl1988}.

In case~\ref{item:exch_rx} knowledge of the long-run frequencies
$F_{\xo\mid\ro}$ cannot give us directly the probability of $\ro$ given
$\xo$. Such probability can be obtained with Bayes's theorem:
\begin{equation}
  \label{eq:p_xr_bayes}
  \p(\ro \| \xo,\ \yF, H_{\textrm{i}}) =
  \frac{
    \p(\xo \| \ro,\ \yF, H_{\textrm{i}}) \ \p(\ro\| H_{\textrm{i}})
  }{
\sum_{r} \p(\xo \| r,\ \yF, H_{\textrm{i}}) \ \p(r\| H_{\textrm{i}})
} =
  \frac{ F_{\xo\mid\ro} \ \p(\ro\|  H_{\textrm{i}})
  }{ \sum_{r} F_{\xo\mid r} \ \p(r\| H_{\textrm{i}})} \ .
\end{equation}
The prior probability $\p(\ro\| H_{\textrm{i}})$ required by the theorem
cannot be obtained from the training set in this case. It needs to be
assessed from other kinds of information or assumptions.


\section{Required probability and approximations}
\label{sec:p_final_inference}

\subsection{Combination of assumptions and final formulae}
\label{sec:combinations_cases_inference}

The assumptions $H_{\textrm{i}}$, $H_{\textrm{i}}$, $H_{\textrm{iii}}$ of
\sect~\ref{sec:hyp_exch} give us the probability $\p(\ro \| \xo \dotsc)$ of
$\ro$ given $\xo$ if the long-run frequencies were known. The assumptions
$H_{\textrm{I}}$, $H_{\textrm{II}}$ of e\sect~\ref{sec:prob_longrun} give
us probabilities for these long-run frequencies. By combining the two kinds
of assumptions we finally arrive at the probability of interest
$\p(\ro \| \xo,\ \rd,\xd,\ H)$.

We consider four combinations of assumptions:
\begin{enumerate}[label=\arabic*.]
\item\label{item:I_ii} $H_{\textrm{I}}$ and $H_{\textrm{ii}}$:
  exchangeability in $r$ given $x$, mutually informative $\yF_{r\q}$ and
  $\yFr$.
  % Combining
  % \eqns~\eqref{eq:p_F_unfactorizable}, \eqref{eq:post_prob_F},
  % \eqref{eq:p_rx_F}
  We find
  \begin{equation}
    \label{eq:post_rx_simple}
    \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{ii\,I}})
=  \int F_{\ro\mid\xo}\ \p(\yF\| \rd,\xd, H_{\textrm{I}})\ \di\yF
%\prod_{i=1}^{N} F_{r_{i}\,x_{i}} \ \di\yF \ .
  % = \frac{\int F_{\ro\mid\xo}\ \p(\yF\| H)\
  %   \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}} \ \di\yF
  % }{\int \p(\yF\| H)\
  %      \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}
  % }
\end{equation}
to be combined with \eqn~\eqref{eq:post_prob_F}.

\item\label{item:I_iii} $H_{\textrm{I}}$ and $H_{\textrm{iii}}$:
  exchangeability in $r$ and $x$, mutually informative $\yF_{r\q}$ and
  $\yFr$. % Combining
  % \eqns~\eqref{eq:p_F_unfactorizable}, \eqref{eq:post_prob_F},
  % \eqref{eq:p_full_F},~\eqref{eq:F_update_xo}
  We find
  \begin{equation}
    \label{eq:post_rx_updated}
        \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{iii\,I}})
        =
        \frac{\int F_{\ro\,\xo}\ \p(\yF\| \rd,\xd, H_{\textrm{I}})\
          \di\yF}{
          \int F_{\q\xo}\ \p(\yF\| \rd,\xd, H_{\textrm{I}})\
          \di\yF
        }
  % = \frac{\int F_{\ro\mid\xo}\ \p(\yF\| H)\
  %   \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}} \ \di\yF
  % }{\int \p(\yF\| H)\
  %      \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}
  % }
  \end{equation}
  to be combined with \eqn~\eqref{eq:post_prob_F}.

\item\label{item:I_i} $H_{\textrm{I}}$ and $H_{\textrm{i}}$:
  exchangeability in $x$ given $r$, mutually informative $\yF_{r\q}$ and
  $\yFr$. Considering \eqn~\eqref{eq:p_xr_bayes} we find
  \begin{equation}
    \label{eq:post_xr_unfactorized}
    \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{i\,I}}) = \frac{
    \p(\ro\|  H_{\textrm{i}})\ \int F_{\xo\mid\ro}\ \p(\yF\| \rd,\xd,
    H_{\textrm{I}})\ \di\yF
  }{
    \sum_{r}\p(r\|  H_{\textrm{i}})\ \int F_{\xo\mid r}\ \p(\yF\| \rd,\xd,
    H_{\textrm{I}})\ \di\yF
    }
%\prod_{i=1}^{N} F_{r_{i}\,x_{i}} \ \di\yF \ .
  % = \frac{\int F_{\ro\mid\xo}\ \p(\yF\| H)\
  %   \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}} \ \di\yF
  % }{\int \p(\yF\| H)\
  %      \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}
  % }
\end{equation}
to be combined with \eqn~\eqref{eq:post_prob_F}.

\item\label{item:II_i} $H_{\textrm{II}}$ and $H_{\textrm{i}}$:
  exchangeability in $x$ given $r$, no mutual information between
  $\yF_{r\q}$ and $\yFr$. Considering \eqns~\eqref{eq:p_xr_bayes}
  and~\eqref{eq:post_prob_Fxcondr} we find
  \begin{equation}
    \label{eq:post_xr_factorized}
    \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{i\,II}}) = \frac{
    \p(\ro\|  H_{\textrm{i}})\ \int F_{\xo\mid\ro}\ \p(\yFr\| \rd,\xd,
    H_{\textrm{II}})\ \di\yFr
  }{
    \sum_{r}\p(r\|  H_{\textrm{i}})\ \int F_{\xo\mid r}\ \p(\yF_{\bm{\mid}r}\| \rd,\xd,
    H_{\textrm{II}})\ \di\yF_{\bm{\mid}r}
    }
%\prod_{i=1}^{N} F_{r_{i}\,x_{i}} \ \di\yF \ .
  % = \frac{\int F_{\ro\mid\xo}\ \p(\yF\| H)\
  %   \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}} \ \di\yF
  % }{\int \p(\yF\| H)\
  %      \prod_{r\,x} {F_{r\,x}}^{N\, f_{r\,x}}
  % }
\end{equation}
to be combined with \eqn~\eqref{eq:post_prob_Fxcondr}.
\end{enumerate}

Combination~\ref{item:I_ii} is used for example in
\textcite{muelleretal1996} and discussed in
\textcite[\sect~4]{quintanaetal2020}.

Combination~\ref{item:I_iii} differs from~\ref{item:I_ii} in that its
probability for the long-run frequencies $\yF$ is updated with the knowledge
of $x$. When the training set is large there should be little difference
between the two cases.

Computationally combination~\ref{item:II_i} is the least costly, because it
requires densities defined in one less dimension, and allows for parallel
use of Markov-chain Monte Carlo samplers, one for each value $r$.

Finally, combinations~\ref{item:I_i} and \ref{item:II_i} can be used only
if $r$ is a discrete variable.


\subsection{Monte Carlo approximations}
\label{sec:mc_approx}

As mentioned in \sect~\ref{sec:prob_longrun}, the probability densities
$\p(\yF\|\rd,\xd, H_{\textrm{I}})\,\di\yF$ and
$\p(\yFr\|\rd,\xd, H_{\textrm{II}})\,\di\yFr$ are effectively represented
by a finite number $T$ of samples $\set{\yF^{(t)}}$ and $\set{\yFr^{(t)}}$
drawn from them via Markov-chain Monte Carlo methods, and integration with
respect to them is approximated by summation over such samples. The
formulae obtained in the previous section then take the following
approximate forms:
\begin{enumerate}[wide,label=\arabic*'.]
\item\label{item:approx_I_ii} $H_{\textrm{I}}$ and $H_{\textrm{ii}}$:
  \begin{equation}
    \label{eq:approx_post_rx_simple}
    \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{ii\,I}})
    \approx  \sum_{t} F^{(t)}_{\ro\mid\xo} \ ,\quad
    \yF^{(t)}\text{ drawn from }\p(\yF\|\rd,\xd, H_{\textrm{I}})\,\di\yF \ .
\end{equation}

\item\label{item:approx_I_iii} $H_{\textrm{I}}$ and $H_{\textrm{iii}}$:
  \begin{equation}
    \label{eq:approx_post_rx_updated}
        \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{iii\,I}}) \approx
        \frac{\sum_{t} F^{(t)}_{\ro\,\xo}}{
          \sum_{t} F^{(t)}_{\q\xo} } \ ,\quad
    \yF^{(t)}\text{ drawn from }\p(\yF\|\rd,\xd, H_{\textrm{I}})\,\di\yF \ .
  \end{equation}

\item\label{item:approx_I_i} $H_{\textrm{I}}$ and $H_{\textrm{i}}$:
  \begin{multline}
    \label{eq:approx_post_xr_unfactorized}
    \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{i\,I}}) \approx \frac{
    \p(\ro\|  H_{\textrm{i}})\ \sum_{t} F^{(t)}_{\xo\mid\ro}
  }{ \sum_{r}\p(r\|  H_{\textrm{i}})\ \sum_{t} F^{(t)}_{\xo\mid r} } \ ,\\
    \yF^{(t)}\text{ drawn from }\p(\yF\|\rd,\xd, H_{\textrm{I}})\,\di\yF \ .
\end{multline}

\item\label{item:approx_II_i} $H_{\textrm{II}}$ and $H_{\textrm{i}}$:
  \begin{multline}
    \label{eq:approx_post_xr_factorized}
    \p(\ro \| \xo,\ \rd,\xd, H_{\textrm{i\,II}}) \approx \frac{
    \p(\ro\|  H_{\textrm{i}})\ \sum_{t} F^{(t)}_{\xo\mid\ro}
  }{ \sum_{r}\p(r\|  H_{\textrm{i}})\ \sum_{t} F^{(t)}_{\xo\mid r}} \ ,\\
    \yFr^{(t)}\text{ drawn from }\p(\yFr\|\rd,\xd,
    H_{\textrm{II}})\,\di\yFr \text{ for each $r$} \ .
\end{multline}
\end{enumerate}

\mynote{\wrench Notes on the Markov-chain Monte Carlo adopted}



\textcolor{white}{If you find this you can claim a postcard from me.}

\section{Selection of features and training data}
\label{sec:sel_features_data}




\section{Evaluation}
\label{sec:evaluation}

\mynote{\wrench Section still in progress}

A short informal discussion on the \enquote{validation} and
\enquote{testing} of probability models.

What does it mean to \enquote{test} a predictive probability model? It does
not make sense to test against the truth of falsity of what's being
predicted for two reasons.

First, probability models ordinarily do not give truths; they gives
probabilities. And we can't compare a probability and a truth. It is also
wrong to require that a model give highest probability to the true case, as
a simple example shows.

Suppose you're going to roll a regular die, and wonder about two exclusive
outcomes: outcome $A$ is 1 to 2, outcome $B$ is 3 to 6. You adopt the
probability model $M$, motivated by symmetry arguments, that assigns
$\P(A \| M) = 2/6$ and $\P(B \| M) = 4/6$. Imagine also that a friend of
yours -- who, like you, knows that the die and the roll procedure are
regular -- uses a different probability model $M'$, with
$\P(A \| M') = 0.999$ and $\P(B \| M') = 0.001$. I believe that you would
ask your friend about the reason of this peculiar probability model: the
prior information that you both have does not seem to support such an
extremely high probability for $A$. Now you roll the die, and obtain 2. The
\enquote{truth} is outcome $A$. Your model $M$ assigned the lower
probability to $A$. You friend's model $M'$ assigned an extremely high
probability to $A$ instead. Would you then say that your friend's model was
\enquote{correct}, and yours \enquote{wrong}?

I hope you agree, by simple common sense, that your model $M$ was the most
\emph{reasonable}, because all evidence available to you and your friend
made $A$ the least probable outcome of the two. This example shows that we
cannot judge a model against the truth of the final outcome. We can only
judge it against the prior evidence we had when it was formulated.

Secondly, we consider a probability model because we don't know the truth.
So we don't have any truth to test it against. If we had that truth, we
would not be using a probability model. You might say \enquote{I can at
  least test the model in cases where I know the truth} -- but how do you
know that the truth of your test cases is the same as the real case? You
don't. If you say \enquote{I suspect them to be similar to the real case},
it means that you have some kind of relevant prior information about the
latter. Then you should use this prior information in formulating your
model (if you don't, you're misusing probability theory).

One danger of judging a model against test cases is that we can end up
modifying it in order to fit them better -- ending up in giving unreliable
predictions for the real case, which may turn out to be very different from
the test ones. % -- we don't know, otherwise we wouldn't be using probability theory


Such \enquote{tests}, however, can also be viewed from a different and more
reasonable perspective. It is often difficult to translate our prior
information into a mathematical formula. We can apply a candidate
mathematical formula thus obtained to cases in which we know what the
reasonable predictions would be, given our prior information. If the
mathematical formula leads to different predictions, then it means that our
mathematical translation was incorrect, and we must find a better one. Good
\citey[\sect~4.3 p.~35]{good1950} called this procedure the \enquote{device
  of imaginary results}.

It is in this last sense that we shall now \enquote{validate} the
hypotheses discussed in \sect~\ref{sec:hypotheses}.

\medskip

\mynote{\wrench Validation plan:

  • Set sampled from the original dataset. This represents the case in
  which the frequencies of $r$, $x$, and all the conditional ones are
  expected to be as in our dataset.

  • Set sampled from the original one but with proportions of $r$ values
  different from those appearing in the original one. This represent the
  case in which the conditional frequencies $F_{x\mid r}$ are expected to
  be as in our dataset, but the frequencies of the different $r$ values and
  the conditional frequencies $F_{r\mid x}$ are not expected to be the
  same.


  If we assume $M$ different values of $r$ and $N$ different values of $x$,
  the set of frequencies
  $\set{F_{r\q}, F_{\q x}, F_{r\mid x}, F_{x\mid r}}$ has $M+N+M N + N M$
  components constrained by the $M N$ identities
  $\set{F_{x\mid r}\ F_{r\q} = F_{r\mid x}\ F_{\q x}}$ and $N + M -1$
  independent normalization conditions. This gives $M N - 1$ independent
  components.

}


%%%% examples use empheq
%   \begin{empheq}[left={\mathllap{\begin{aligned}    \de\yF_{\yc}/\de\yp&=0\text{:} \\
%         \de\yF_{\yc}/\de\ym&=0\text{:}\\ \de\yF_{\yc}/\de\yl&=0\text{:}\end{aligned}}\qquad}\empheqlbrace]{align}
%     \label{eq:con_p}
% %    \de\yF_{\yc}/\de\yp &\equiv
%     -\ln\yp + \ln\yq + \yl\yM + \ym\yu &=0,\\
%     \label{eq:con_u}
% %    \de\yF_{\yc}/\de\ym &\equiv
%     \yu\yp-1 &=0,\\
%     \label{eq:con_l}
%     %\de\yF_{\yc}/\de\yl &\equiv
%     \yM\yp-\yc &=0.
%   \end{empheq}
%%%%
% \begin{empheq}[box=\widefbox]{equation}
%   \label{eq:maxent_question}
%   \p\bigl[\yE{N+1}{k} \bigcond \tsum\yo\yf{N}\in\yA, \yM\bigr] = \mathord{?}
% \end{empheq}



% \[
%   \begin{tikzcd}
%       M_{n,n}(\CC) \arrow{r}{R'_{a}(\Hat{U})} & M_{n,n}(\CC)
%     \\
%     L(\mathcal{H}) \arrow{r}{\Hat{U}} \arrow[swap]{d}{R_*}\arrow[swap]{u}{R'_*} & L(\mathcal{H}) \arrow{d}{R_*}\arrow{u}{R'_*} \\
%       M_{n,n}(\CC) \arrow{r}{R_{a}(\Hat{U})} & M_{n,n}(\CC)
%   \end{tikzcd}
% \]

% \[
%   \begin{tikzcd}
%       \CC^n \arrow{r}{R'_*(A)} & \CC^n
%     \\
%     \mathcal{H} \arrow{r}{A} \arrow[swap]{d}{R}\arrow[swap]{u}{R'} & \mathcal{H} \arrow{d}{R}\arrow{u}{R'} \\
%       \CC^n \arrow{r}{R_*(A)} & \CC^n
%   \end{tikzcd}
% \]


% \[
%   \begin{tikzcd}
%     \mathcal{H} \arrow{r}{A} \arrow[swap]{d}{R} & \mathcal{H} \arrow{d}{R} \\
%       \CC^n \arrow{r}{R_*(A)} & \CC^n
%   \end{tikzcd}
% \]

%%\setlength{\intextsep}{0ex}% with wrapfigure
%%\setlength{\columnsep}{0ex}% with wrapfigure
%\begin{figure}[p!]% with figure
%\begin{wrapfigure}{r}{0.4\linewidth} % with wrapfigure
%  \centering\includegraphics[trim={12ex 0 18ex 0},clip,width=\linewidth]{maxent_saddle.png}\\
%\caption{caption}\label{fig:comparison_a5}
%\end{figure}% exp_family_maxent.nb


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Acknowledgements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse
\begin{acknowledgements}
  \ldots to Mari \amp\ Miri for continuous encouragement and affection, and
  to Buster Keaton and Saitama for filling life with awe and inspiration.
  To the developers and maintainers of \LaTeX, Emacs, AUC\TeX, Open Science
  Framework, R, Python, Inkscape, Sci-Hub for making a free and impartial
  scientific exchange possible.
  % Our work was supported by the Trond Mohn Research Foundation, grant number BFS2018TMT07
%\rotatebox{15}{P}\rotatebox{5}{I}\rotatebox{-10}{P}\rotatebox{10}{\reflectbox{P}}\rotatebox{-5}{O}.
%\sourceatright{\autanet}
\mbox{}\hfill\autanet
\end{acknowledgements}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Appendices
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage
\bigskip
%\renewcommand*{\appendixpagename}{Appendix}
%\renewcommand*{\appendixname}{Appendix}
% %\appendixpage
\appendix
\mynote{\wrench Sections still in progress}

\section{Exchangeability, approximations, machine learning}
\label{sec:exchang}

Exchangeability is
the basic fact or assumption upon which machine-learning algorithms and the
calculations presented here are based.

Exchangeability is the fact or assumption that the ordering of
known and yet unknown datapoints is irrelevant for new inferences. Good
reviews are given by \textcite{dawid2013} and
\textcite[\sect~4.2]{bernardoetal1994}.

A probability distribution $\p(z_{0}, z_{1}, z_{2}, \dotsc \| H)$ is called
\emph{infinitely exchangeable} if it's invariant under permutations of the
values $z_{0}, z_{1}, \dotsc$, no matter what and how many they are. In
other words, the ordering of the observations doesn't matter. Usually one
says, somewhat improperly, that \enquote{$z_{0}, z_{1}, \dotsc$ are
  exchangeable} for short. In our case each $z$ is actually a pair of
values $(r,x)$, where $x$ itself is multidimensional.

Exchangeability has the following important consequence: if we knew the
infinite-limit long-run relative frequencies $\yF\defd (F_{r,x})$ of the
values of past and future datapoints, then the probability of observing a
sequence of new values would be equal to the long-run relative frequency of
those values, for symmetry reasons. For example:
\begin{equation}
  \label{eq:prob_freq}
  \p(r_{1},x_{1},\ r_{2},x_{2} \| \yF, H) = F_{r_{1},x_{1}} \
  F_{r_{2},x_{2}} \ .
\end{equation}

Another consequence is that conditional probabilities are also equal to the
long-run \emph{conditional} frequencies $\yFrx \defd (F_{r \|x}) =
(F_{r,x}/\sum_{y'}F_{y',x})$. For example:
\begin{equation}
  \label{eq:prob_cond_freq}
  \p(r_{1}, r_{2}\| x_{1}, x_{2},\ \yF, H) = F_{r_{1}\|x_{1}} \
  F_{r_{2}\|x_{2}} \ .
\end{equation}

If the long-run frequencies $\yF$ are unknown, they can be marginalized,
\ie\ integrated out, provided that we have a probability distribution
$\p(\yF \| H)$ for them. For example:
\begin{equation}
  \label{eq:prob_freq_integr}
  \p(r_{1},x_{1},\ r_{2},x_{2}\| H) = \int F_{r_{1},x_{1}} \
  F_{r_{2},x_{2}} \ \p(\yF \| H)\ \di\yF \ ,
\end{equation}
from which we can also obtain conditional probabilities.

There are several other types of exchangeability. For example, we may have
\emph{conditional} or \emph{partial} exchangeability of $r$ given $x$ if
\eqn~\eqref{eq:prob_cond_freq} holds, but not \eqn~\eqref{eq:prob_freq}. If
we have a probability distribution $\p(\yFrx \| H)$ for the conditional
frequencies, we obtain by marginalization
\begin{equation}
  \label{eq:prob_cond_freq_integr}
  \p(r_{1}, r_{2}\| x_{1}, x_{2},\ H) = \int F_{r_{1}\|x_{1}} \
  F_{r_{2}\|x_{2}}\ \p(\yFrx \| H)\ \di\yFrx \ .
\end{equation}

\textcite{lindleyetal1981} discuss the importance of conditional
exchangeability for various inference problems, and the errors that arise
if the wrong type of exchangeability is assumed.

From the generalization of the equations above we obtain other
probabilities for various cases of regression, such as
\eqns~\eqref{eq:main_prob} or \eqref{eq:rev_prob}, by simply using the
three rules of the probability calculus. For example, if $\ro$ is assumed
to be exchangeable given $\xo$, but the latter is not assumed to be
exchangeable with the other $\xd$ because hand-picked, we find
\begin{equation}
  \label{eq:main_prob_exch}
  \p(\ro \| \xo,\ \rd,\xd,\ H) =
  \frac{
    \int F_{\ro\|\xo} \ F_{r_{1},x_{1}} \  \dotsm \
  F_{r_{N},x_{N}} \ \p(\yF \| H)\ \di\yF
  }{
    \int F_{r_{1},x_{1}} \  \dotsm \
  F_{r_{N},x_{N}} \ \p(\yF \| H)\ \di\yF
  } \ .
\end{equation}

The distribution $\p(\yF \| H)$, usually called the \enquote{prior},
embodies the assumptions that we make for the extrapolation. It therefore
cannot be determined by the known datapoints. Given enough known datapoints
$(\rd,\xd)$, the probability distribution for $\ro$ given $\xo$
eventually does become equal to their limit conditional frequency. This
sets the ultimate uncertainty with which the prediction can be made, and
cannot be surpassed by any algorithm. So this approach eventually does
yield an optimal inference. The quickness with which the limit conditional
frequency is reached, however, depends heavily on the choice of prior. The
prior should therefore be chosen in a well-reasoned manner. In practice the
choice is limited by computational constraints.

All machine-learning algorithms calculate equations such as the one above
or approximations to it, for some choice of the prior $\p(\yF \| H)$
\parencites{mackay1992,bishop2006}.

The formulae above can be numerically implemented exactly or with a good
approximation only if the features $x$ have low dimensions and the number
$N$ of known data is small \mynote{\wrench\ add refs}. Many
machine-learning algorithms manage to deal with larger dimensionality and
datapoints by parametrizing the $\yF$-space in a clever way (for example,
deep nets parametrize the $\yF$ as nested compositions of some simple
functions) and finding an argument that maximizes the integrands above
\parencites{mackay1992,mackay1992b}, for example a value of $\ro$ that
locally maximizes the integrand in the numerator of
\eqn~\eqref{eq:main_prob_exch}. This means, though, that they cannot
quantify the uncertainty of the inference.

\section{Exchangeability assumptions in formulae}
\label{sec:exch_formulae}


\begin{enumerate}[label=\roman*.]
\item\label{item:exch_rx}\emph{Exchangeability of \rmsd\ given features}.
  The \rmsd s of the training set are relevant to the inference of the
  \rmsd\ of the new datapoint, given the same feature values. The features
  of the training set, however,

  Exchangeability of $r$ given $x$: The joint
  probability of the \rmsd s of new datapoint and training set is symmetric
  under exchanges of datapoints, given the same features. That is,
  \begin{multline}
    \label{eq:p_exch_rx}
    \begin{aligned}
    &\p(r=a_{0}, r_{1}=a_{1}, r_{2}=a_{2}, \dotsc \|
    x_{1}=b, x_{2}=b, x_{3}=b, \dotsc,\ H) ={}\\
    &\p(r=a_{1}, r_{1}=a_{0}, r_{2}=a_{2}, \dotsc \|
    x_{1}=b, x_{2}=b, x_{3}=b, \dotsc,\ H) ={}\\
    &\p(r=a_{2}, r_{1}=a_{1}, r_{2}=a_{0}, \dotsc \|
    x_{1}=b, x_{2}=b, x_{3}=b, \dotsc,\ H) = \dotso
    \end{aligned} \\
    \text{and so on, for all simultaneous permutations of $a_{i}$ and any $b$.}
  \end{multline}

\item\label{item:exch_xr} Exchangeability of $x$ given $r$: The joint
  probability of the features of new datapoint and training set is symmetric
  under exchanges of datapoints, given the same \rmsd. That is,
  \begin{multline}
    \label{eq:p_exch_xr}
    \begin{aligned}
    &\p(x=b_{0}, x_{1}=b_{1}, x_{2}=b_{2}, \dotsc \|
    r_{1}=a, r_{2}=a, r_{3}=a, \dotsc,\ H) ={}\\
    &\p(x=b_{1}, x_{1}=b_{0}, x_{2}=b_{2}, \dotsc \|
    r_{1}=a, r_{2}=a, r_{3}=a, \dotsc,\ H) ={}\\
    &\p(x=b_{2}, x_{1}=b_{1}, x_{2}=b_{0}, \dotsc \|
    r_{1}=a, r_{2}=a, r_{3}=a, \dotsc,\ H) = \dotso
    \end{aligned} \\
    \text{and so on, for all simultaneous permutations of $b_{i}$ and any $a$.}
  \end{multline}
\item\label{item:exch_full} The \rmsd s and features of the training set
  are jointly relevant to the inference of those of the new datapoint. We
  then have full exchangeability of $(r,x)$: the joint probability of new
  datapoint and training set is symmetric under exchanges of datapoints.
  That is,
  \begin{multline}
    \label{eq:p_exch_full}
    \begin{aligned}
    &\p(r=a_{0}, x=b_{0},\ r_{1}=a_{1},x_{1}=b_{1}, \
    r_{2}=a_{2},x_{2}=b_{2}, \ \dotsc \| H) ={}\\
    &\p(r=a_{1}, x=b_{1},\ r_{1}=a_{0},x_{1}=b_{0}, \
    r_{2}=a_{2},x_{2}=b_{2}, \ \dotsc \| H) ={}\\
    &\p(r=a_{2}, x=b_{2},\ r_{1}=a_{1},x_{1}=b_{1}, \
      r_{2}=a_{0},x_{2}=b_{0}, \ \dotsc \| H) = \dotso
    \end{aligned} \\
    \text{and so on, for all simultaneous permutations of $a_{i}$ and $b_{i}$.}
  \end{multline}

\end{enumerate}



\section{Selection of prior}
\label{sec:sel_prior}

The prior densities $\p(\yF\| \rd, \xd, H_{\textrm{I}})\,\di\yF$ and
$\p(\yFr\| \rd, \xd, H_{\textrm{II}})\,\di\yFr$ for each $r$ are
represented by a Dirichlet-process mixture with product kernel of a
multivariate normal for the continuous quantities and Dirichlet
distributions for the discrete ones. As the average measure resulting from
the Dirichlet process we choose the product of a normal-inverse-Wishart
distribution and Dirichlet distributions; this choice leads to faster
computation.

Let us first focus on the continuous features \enquote{tanimoto} and
\enquote{sasa}.

The normal-inverse-Wishart has four parameters
$\mu, \varSigma, \kappa, \delta$, and leads to a multivariate
t-distribution for the quantities. The mean of the t-distribution is equal
to $\mu$; the expected value of the covariance matrix is
$\frac{1}{\delta-2}\varSigma$; the covariance of the t-distribution is
$\frac{\kappa+1}{\kappa\ (\delta-2)}\varSigma$; the covariance of the mean
is $\frac{1}{\kappa\ (\delta-2)}\varSigma$; the degrees of freedom of the
t-distribution equal $\delta$.

We choose these parameters, the concentration parameter of the Dirichlet
process, and the coordinate systems on the space of the continuous
quantities in order to build a prior distribution that represents as well
as possible our knowledge before seeing the data. Here are some
requirements:
\begin{enumerate}[label=(\alph*)]
\item The \enquote{sasa} quantities $x$ have a range in $\clop{0,+\infty}$.
  We consider them as scale quantities (even if the value $0$ is included)
  and transform them to a log-scale:
    \begin{equation}
    \label{eq:log_sasa}
    x \mapsto r = \tfrac{1}{4}\ln x \ .
  \end{equation}
  Their prior distribution is chosen as almost uniform on this scale:
  $\p[\ln(x) \| H]\ \di\ln(x) \propto \di\ln(x)$, approximated by a
  t-distribution with large standard deviation.

\item We set the mean of the t-distribution for the \enquote{sasa}
  quantities equal to the mean observed in the full dataset, in order to represent
  theoretical prior knowledge on this kind of quantities.

\item The \enquote{tanimoto} quantities $x$ have a range in $\clcl{0,1}$.
  In this coordinate system we assume that they have a Jeffreys prior $\p(x
  \| H)\ \di x \propto \frac{\di x}{x\ (1-x)}$.
  We transform them to $\RR$ with a logit function
  \begin{equation}
    \label{eq:logit_tanimoto}
    x \mapsto r = \tfrac{1}{2}\logit(x) \defd \tfrac{1}{2}\ln\frac{x}{1-x} \ .
  \end{equation}
In this coordinate system the prior is uniform, approximated by a
t-distribution with large standard deviation.

\item We want the priors for the \enquote{sasa} and \enquote{tanimoto}
  quantities to be approximately independent; this is obtained by choosing
  a large degree-of-freedom parameter for the multivariate t-distribution.
\item We choose the expected variances for the \enquote{sasa} and
  \enquote{tanimoto} quantities mapped to $\RR$ to be approximately equal
  to the variances observed in the full dataset, in order to represent
  theoretical prior knowledge on this kind of quantities.
\item We choose the standard deviations for the means of the \enquote{sasa}
  and \enquote{tanimoto} quantities to be approximately equal to their
  ranges observed in the full dataset, in order to represent theoretical
  prior knowledge on this kind of quantities.
\end{enumerate}

\begin{equation}
  \label{eq:requirements_hyperparams}
  \begin{aligned}
   &\frac{\kappa+1}{\kappa\ (\delta-2)}\varSigma = 5^{2}
&&\text{\small(t-distr. variance)}
    \\
    &\frac{1}{\kappa\ (\delta-2)}\varSigma = 1^{2}
&&\text{\small(variance of mean)}
    \\
    &\frac{1}{\delta-2}\varSigma = \frac{1}{2^{2}}
&&\text{\small(exp. variance)}
    \\
    &\mu_{\text{sasa}} = 1 &&
    \\
    &\delta = 30 &&
  \end{aligned}
\end{equation}

*****



based on a Dirichlet-process mixture with
product kernels of multivariate normal and Dirichlet distributions.



The \enquote{sasa} quantities $x$ have a range in $\clop{0,+\infty}$. We
consider them as scale quantities (even if the value $0$ is included) and
transform them to a log-scale. Their prior distribution is chosen as
almost uniform on this scale:
$\p(\ln(x) \| H)\ \di\ln(x) \propto \di\ln(x)$, approximated by a normal
with large standard deviation.

The \enquote{tanimoto} quantities $x$ have a range in $\clcl{0,1}$. We
transform them to $\RR$ using the cumulative normal distribution:
\begin{equation}
  \label{eq:transf_tanimoto}
  x \mapsto \phi(x) \defd
  \frac{1}{2}\operatorname{erf}\biggl(\frac{x}{\sqrt{2}}\biggr)
  \equiv \frac{1}{\sqrt{2\pu}}\int_{-\infty}^{x} \e^{-t^{2}/2}\ \di t \ ,
\end{equation}
which we call an \defquote{erf-scale}.
The prior is chosen as uniform in $\di x$, and therefore has a standard
normal density in the erf-scale.

Choosing a normal-inverse-Wishart distribution with parameters
$\mu^{*}, \varDelta^{*}, \kappa^{*}, \nu^{*}$ as the mean for the Dirichlet
process leads to a t-distribution as the predictive distribution for a
quantity $x \in \RR^{d}$, with $\nu^{*}-n+1$ degrees of freedom, mean
$\mu^{*}$, covariance
$\frac{\kappa^{*}+1}{\kappa^{*}\ (\nu^{*}-d-1)}\varDelta^{*}$.

This means that we want $\nu^{*}-n+1$ large, say ${}\sim 10$, in order to
approximate a normal, $\mu^{*}$ equal to the zero vector, and
$\frac{\kappa^{*}+1}{\kappa^{*}\ (\nu^{*}-d-1)}\varDelta^{*}$ diagonal and
equal to $1$ in the direction of the \enquote{tanimoto} quantities, and
equal to some larger value, say ${}\sim 4$, in the direction of the \enquote{sasa}
quantities. These requirement leave the parameter $\kappa^{*}$ still undefined.

\iffalse
\section{Alternative formulations}
\label{sec:alt_form}

\begin{equation}
  F(r \| x) = \sum_{k} q_{k}(x)\ N[r \| \mu_{k}(x), \sigma_{k}(x)]
\end{equation}


\begin{equation}
  \p(F_{\bm{\mid}}  \| H)\ \di F_{\bm{\mid}} =
  \p(\bm{q}, \bm{\mu}, \bm{\sigma} \| H)\
  \di\bm{q}\ \di\bm{\mu}\ \di\bm{\sigma}
\end{equation}
with an infinite-dimensional normal distribution (Gaussian process).

\begin{multline}
  \p(r \| x, H) = \int F(r \| x)\ \p(F_{\bm{\mid}} \| H)\ \di F_{\bm{\mid}} ={}\\
  \iiint \sum_{k} q_{k}(x)\ N[r \| \mu_{k}(x), \sigma_{k}(x)]\
  \p(\bm{q}, \bm{\mu}, \bm{\sigma} \| H)\
  \di\bm{q}\ \di\bm{\mu}\ \di\bm{\sigma}
\end{multline}


\begin{multline}
  \p(F_{\bm{\mid}} \| \rd, \xd, H) =
  \frac{\p(\rd \| F_{\bm{\mid}},\ \xd, H)\ \p(F_{\bm{\mid}} \| H)
  }{
  \int  \p(\rd \| F_{\bm{\mid}},\ \xd, H)\ \p(F_{\bm{\mid}} \| H)\ \di F_{\bm{\mid}}
} ={}\\[2\jot]
\frac{
  \prod_{i}\bigl\{
  \sum_{k} q_{k}(x_{i})\ N[r_{i} \| \mu_{k}(x_{i}),
  \sigma_{k}(x_{i})]
  \bigr\} \
  \p(\bm{q}, \bm{\mu}, \bm{\sigma} \| H)
}{
\iiint   \sum_{k} q_{k}(x)\ N[r \| \mu_{k}(x), \sigma_{k}(x)] \
\p(\bm{q}, \bm{\mu}, \bm{\sigma} \| H)\
\di\bm{q}\ \di\bm{\mu}\ \di\bm{\sigma}
}
\end{multline}
\fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Bibliography
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\renewcommand*{\finalnamedelim}{\addcomma\space}
\defbibnote{prenote}{{\footnotesize (\enquote{de $X$} is listed under D,
    \enquote{van $X$} under V, and so on, regardless of national
    conventions.)\par}}
% \defbibnote{postnote}{\par\medskip\noindent{\footnotesize% Note:
%     \arxivp \mparcp \philscip \biorxivp}}

\printbibliography[prenote=prenote%,postnote=postnote
]

\clearpage
\mynote{\wrench OLD TEXT BELOW}\\
**********************************************************************



We introduce the long-run, joint relative frequencies $F_{r\,x}$ that would
be observed if the training dataset could hypothetically be extended to an
infinite size. With these we also have
marginal and conditional frequencies $F_{r\q}$, $F_{\q x}$, $F_{r\mid x}$,
$F_{x \mid r}$. These frequencies are obviously unknown but they play a
pivotal role in the inferences we are interested in. The collection of
joint frequencies $\bigl(F_{r\,x}\bigr)_{r\,x}$ will be denoted by $\yF$.


In the following $r$ will be treated as a discrete quantity with three
possible values: \enquote*{good}
($0\ \textrm{\AA}\le r \le 2\ \textrm{\AA}$), \enquote*{inconclusive}
($2\ \textrm{\AA} < r < 3\ \textrm{\AA}$), \enquote*{bad}
($3\ \textrm{\AA} \le r$).

We denote with $f_{r\, x}$ the joint relative frequency for values $r$ and
$x$ in the training dataset; with $f_{r\q}$ the marginal relative frequency
for $r$, and with $f_{\q x}$ that for $x$; with $f_{r\mid x}$ the
conditional relative frequency for $r$ given $x$, and vice versa for
$f_{x \mid r}$. These frequencies are related by
\begin{equation}
  \label{eq:frequencies_relations}
  f_{r\q}=\sum_{x} f_{r\,x}\ , \quad
  f_{\q x}=\sum_{r} f_{r\,x}\ ,\quad
  f_{r\mid x}=f_{r\,x}/f_{\q x}\ , \quad
  f_{x\mid r}=f_{r\,x}/f_{r\q}\ .
\end{equation}

We introduce the long-run, joint relative frequencies $F_{r\,x}$ that would
be observed if the training dataset could hypothetically be extended to an
infinite size\footnote{\enquote{But this \emph{long run} is a misleading
    guide to current affairs. \emph{In the long run} we are all dead.}
  \citep[\sect~3.I, p.~65]{keynes1923_r2013}}. With these we also have
marginal and conditional frequencies $F_{r\q}$, $F_{\q x}$, $F_{r\mid x}$,
$F_{x \mid r}$. These frequencies are obviously unknown but they play a
pivotal role in the inferences we are interested in. The collection of
joint frequencies $\bigl(F_{r\,x}\bigr)_{r\,x}$ will be denoted by $\yF$.

\bigskip



The three cases of exchangeability listed above lead to three different
equations for the calculation of the probability~\eqref{eq:main_prob}.
Suppose that our uncertainty about the long-run frequencies in quantified
by the probability density $\p(\yF\|\rd,\xd, H)\,\di\yF$. Then:
\begin{enumerate}[label=\Roman*.]
\item In case~\ref{item:exch_rx} we have
  \begin{equation}
    \label{eq:prob_rx_case_rx}
    \p(\ro \| \xo,\ \rd,\xd,\ H) =
    \int F_{\ro\mid\xo} \ \p(\yF\|\rd,\xd, H)\ \di\yF \ .
  \end{equation}
\item In case~\ref{item:exch_xr} we have
  \begin{equation}
    \label{eq:prob_rx_case_xr}
    \p(\ro \| \xo,\ \rd,\xd,\ H) =
    \frac{
     \p(\ro\| H)\  \int F_{\xo\mid\ro} \ \p(\yF\|\rd,\xd, H)\ \di\yF
    }{
     \sum_{r'} \p(r' \| H)\  \int F_{\xo\mid r'} \ \p(\yF\|\rd,\xd, H)\ \di\yF
      } \ .
  \end{equation}
\item In case~\ref{item:exch_full} we have
  \begin{equation}
    \label{eq:prob_rx_case_full}
    \p(\ro \| \xo,\ \rd,\xd,\ H) =
    \frac{
     \int F_{\ro\,\xo} \ \p(\yF\|\rd,\xd, H)\ \di\yF
    }{
     \int F_{\q\xo} \ \p(\yF\|\rd,\xd, H)\ \di\yF
      } \ .
  \end{equation}
\end{enumerate}

For example, if $\ro$ is
hand-picked by someone, then knowledge of the frequencies for $(\rd,\xd)$
is irrelevant. But even in such a case it might be that the reversed
probability $\p(\xo \| \ro,\ \rd,\xd,\ H)$ \emph{does} depend on the
frequencies of previous datapoints, and it can be used to
calculate~\eqref{eq:main_prob} as
\begin{equation}
  \label{eq:rev_prob}
  \p(\ro \| \xo,\ \rd,\xd,\ H) =
  \frac{\p(\xo \| \ro,\ \rd,\xd,\ H)\ \p(\ro \| H)}{
    \sum_{\ro}\p(\xo \| \ro,\ \rd,\xd,\ H)\ \p(\ro \| H)}
\ .
\end{equation}
The important difference between \eqns~\eqref{eq:main_prob} and
\eqref{eq:rev_prob} is that in the latter we must also quantify
$\p(\ro \| H)$ using extra-data facts or hypotheses.

In technical terms we are asking whether $\ro$ is \emph{exchangeable}
given $\xo$, or vice versa. Using Fisherian \citey[\sects~II.4,
IV.1]{fisher1956} parlance we are asking whether $\ro$ should be
considered as belonging to a \emph{subpopulation} determined by $\xo$ or
vice versa. (Or neither, in which case our study would simply end here, so
we won't consider this third possibility.) Another way of seeing our
question is whether \enquote{causal connections} (which can originate from
a common cause) are more robust in the direction
$\xo \rightsquigarrow \ro$ or $\ro \rightsquigarrow \xo$
\parencites[\cf][\sects~2.1.2, 2.2.5]{pearl1988}.


We study both possibilities mentioned above. At the end we'll compare the
results and discuss which assumption makes more sense in various
applications.

The final goal is to compare the results of the present principled approach
with those of machine-learning algorithms in a case where both can be used.
More about this in \mynote{\wrench\ \sect***}.


\section{Direct case: methodology}
\label{sec:direct_method}

For the direct case~\eqref{eq:main_prob} we consider the \rmsd\ $r$ as a
continuous variable, mapped to a log-scale to avoid dealing with finite
ranges.

In the direct case the assumption is that $\ro$ is exchangeable given
$\xo$, but $\xo$ is not exchangeable given the known datapoints $\xd$.
This is a sensible assumption -- a fact indeed -- if we are \emph{choosing}
the specific value of $\xo$, as we do in some testing situations for
example. The values $(\rd,\xd)$ are considered exchangeable.

The probability $\p(\ro \| \xo,\ \rd,\xd,\ H)$ is then given by
\eqn~\eqref{eq:main_prob_exch}, which can be rewritten this way:
\begin{equation}
  \label{eq:direct_case_main}
  \begin{gathered}
  \p(\ro \| \xo,\ \rd,\xd,\ H) =
  \int F_{\ro\|\xo} \ \p(\yF \| \rd, \xd,\ H)\ \di\yF
  \\[\jot]
  \text{with}\quad
  \p(\yF \| \rd, \xd,\ H) =
  \frac{
    \ F_{r_{1},x_{1}} \  \dotsm \
  F_{r_{N},x_{N}} \ \p(\yF \| H)
  }{
    \int F_{r_{1},x_{1}} \  \dotsm \
  F_{r_{N},x_{N}} \ \p(\yF \| H)\ \di\yF
  }  \ ,
\end{gathered}
\end{equation}
which is the main formula of the direct case.

The integrals are over the set of long-run joint frequency distributions,
which is an infinite-dimensional manifold. A currently popular way to
parametrize it and at the same time choose a prior $\p(\yF \| H)$ is by
means of so-called Dirichlet-process mixtures. In short, a generic
$F_{r,x}$ is represented as a countable weighted sum of a simpler
distribution $K$, called the kernel, with different parameters:
$F_{r,x} = \sum_{i}w_{i} K(r,x \| \theta_{i})$. The prior is therefore
defined over the possible infinite tuples $(w_{i},\theta_{i})$. A Dirichlet
process is chosen as such prior. In the present case the kernel is the
product of a multivariate normal distribution for $r$ and for the
continuous features in $x$, and a Dirichlet distribution for the discrete
features in $x$. An example of
this approach is analysed and used by \textcites{muelleretal1996}. A
discussion of why such a choice of prior may not be sensible is discussed
by \textcite{petrone2017} and \textcite[\sect~4]{quintanaetal2020}.



The first integral in \eqn~\eqref{eq:direct_case_main} is numerically
approximated by a sum of values of $F_{\ro \| \xo}$ sampled from the
distribution $\p(\yF \| \rd, \xd,\ H)$ via Markov-chain Monte Carlo
methods. Details about the sampling algorithm are given in \mynote{\wrench\
  \sect***}.

\section{Reverse case: methodology}
\label{sec:reverse_method}

For the reverse case~\eqref{eq:rev_prob} we consider a binned \rmsd\
divided into three categories:
$r \in \set{\text{\enquote*{good}},\, \text{\enquote*{inconclusive}},\,
  \text{\enquote*{bad}}}$ .

In the reverse case the assumption is that $\xo$ is exchangeable given
$\ro$, but $\ro$ is not exchangeable given the known datapoints $\rd$.
This is again a sensible assumption if we are choosing the specific value
of $\xo$, as we do in testing situations, because then $\ro$ cannot be
considered to come from some unsystematic process, if it is has some causal
connections with $\xo$. The values $(\rd,\xd)$ are considered
exchangeable.

The probability $\p(\xo \| \ro,\ \rd,\xd,\ H)$ is obtained analogously
to \eqn~\eqref{eq:prob_cond_freq_integr}, reversing the roles of $r$ and
$x$. We make the additional assumption that the prior
$\p(\yFxr \| H)$ factorizes for the frequencies conditional on the three
$r$ categories:
\begin{equation}
  \label{eq:factor_prior_cond}
  \p(\yFxr \| H) =
  \p(\yF_{\bm{x \mid} \text{good}} \| H)\cdot
  \p(\yF_{\bm{x \mid} \text{inconcl.}} \| H)\cdot
  \p(\yF_{\bm{x \mid} \text{bad}} \| H)\ .
\end{equation}
The result is three distinct conditional probabilities:
\begin{equation}
  \label{eq:reverse_case_main}
  \begin{gathered}
  \p(\xo \| r,\ \rd,\xd,\ H) =
\int F_{\xo\|r} \ \p(\yF_{\bm{x\mid}r} \| \xd,\ H)\ \di\yF_{\bm{x\mid}r}
  \\[\jot]
  \p(\yF_{\bm{x\mid}r} \| \xd,\ H) =
  \frac{
    \ F_{x_{1'}\| r} \  \dotsm \
  F_{x_{N'}\| r} \ \p(\yF_{\bm{x\mid}r}  \| H)
  }{
    \int
    \ F_{x_{1'}\| r} \  \dotsm \
  F_{x_{N'}\| r} \ \p(\yF_{\bm{x\mid}r}  \| H)\ \di\yF_{\bm{x\mid}r}
}\\[\jot]
  \text{for each } r \in
  \set{\text{\enquote*{good}},\, \text{\enquote*{inconclusive}},\,
    \text{\enquote*{bad}}}\ ,
\end{gathered}
\end{equation}
where the conditionals contain only $\xd$ values associated with that
specific $r$ value \mynote{(\wrench\ I'll find clearer notation and
  explanations)}.

The fact that we have three distinct conditional probability distributions
allows us to split the computation into three. They can be done in parallel
and each has one less dimension and is being conditioned on fewer data, so
the computation is somewhat faster.

The computation of each probability is analogous to that explained in
\sect~\ref{sec:direct_method}, based on a Dirichlet-process mixture with
product kernels of multivariate normal and Dirichlet distributions.


%%run: 560 s in parallel, 20e3 burn-in, 10e3 sweeps, 10 thinning


\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% Cut text (won't be compiled)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The first question we must ask to find theis this: Are the
various long-run frequencies $F_{r\,x}$ \etc\ relevant to our uncertainty
about $\ro$ and $\xo$? We consider three mutually exclusive cases:
\begin{enumerate}[label=\roman*.]
\item\label{item:exch_rx} The conditional frequency $F_{\ro\mid \xo}$ of
  $\ro$ given $\xo$ is relevant to the inference of $\ro$ given $\xo$. The
  marginal frequency $F_{\q \xo}$, however, is irrelevant to the inference
  of $\xo$.
\item\label{item:exch_xr} The conditional frequency $F_{\xo\mid \ro}$ of
  $\xo$ given $\ro$ is relevant to the inference of $\xo$ given $\ro$. The
  marginal frequency $F_{\ro\q}$, however, is irrelevant to the inference
  of $\ro$.
\item\label{item:exch_full} The joint frequency $F_{\ro\, \xo}$ of $\ro$
  and $\xo$ is relevant to the joint inference of both. This is equivalent
  to the relevance of $F_{\ro\mid \xo}$ and $F_{\q \xo}$, and of
  $F_{\xo\mid \ro}$ and $F_{\ro\q}$.
\end{enumerate}
Either marginal frequency $F_{\ro\q}$ or $F_{\q\xo}$ could be irrelevant
because the respective quantity could be chosen by hand or selected by some
process different from that underlying that frequency.

If $F_{\ro\q}$ is relevant, in technical terms we say that $\ro$ (or more
precisely its probability) is \emph{exchangeable} with all $\rd$ in the
training dataset. Or we can say that $\ro$ belongs to the same population
as the $\rd$ do. If $F_{\ro\mid \xo}$ is relevant, we say that $\ro$ is
exchangeable with the $\rd$ in the training dataset \emph{given $\xo$}. Or
we can say that $\ro$ belongs to the same subpopulation of $\rd$
characterized by a joint value $\xo$. Similarly for the converse cases.

%%% Local Variables:
%%% mode: LaTeX
%%% TeX-PDF-mode: t
%%% TeX-master: t
%%% End:
